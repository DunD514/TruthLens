{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "zHWymVT3LxZd",
        "outputId": "be8fb9f0-10bf-4499-c3b0-d80989ce9a1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upload an image to predict:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-38b783ec-7f96-414b-a98e-c27bc4b7914f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-38b783ec-7f96-414b-a98e-c27bc4b7914f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install -q transformers torch torchvision pillow ipywidgets\n",
        "from transformers import AutoImageProcessor, SiglipForImageClassification\n",
        "from torch.optim import AdamW\n",
        "from PIL import Image\n",
        "from google.colab import files, drive\n",
        "import torch\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "MODEL_PATH = \"/content/drive/MyDrive/deepfake_detector_model_v1\"\n",
        "\n",
        "# Load model or pretrained\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    print(\"Loading model from Google Drive...\")\n",
        "    model = SiglipForImageClassification.from_pretrained(MODEL_PATH)\n",
        "else:\n",
        "    print(\"Loading model from pretrained hub...\")\n",
        "    model_name = \"prithivMLmods/deepfake-detector-model-v1\"\n",
        "    model = SiglipForImageClassification.from_pretrained(model_name)\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(\"prithivMLmods/deepfake-detector-model-v1\")\n",
        "model.train()\n",
        "\n",
        "id2label = {0: \"FAKE\", 1: \"REAL\"}\n",
        "\n",
        "output = widgets.Output()\n",
        "\n",
        "def fine_tune_on_example(image, correct_label):\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    labels = torch.tensor([correct_label]).unsqueeze(0)  # batch size 1\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-6)\n",
        "\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(**inputs, labels=labels)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Save model after fine-tuning\n",
        "    model.save_pretrained(MODEL_PATH)\n",
        "    processor.save_pretrained(MODEL_PATH)\n",
        "\n",
        "    print(f\"Model fine-tuned on example with label: {id2label[correct_label]}, loss: {loss.item():.4f}\")\n",
        "    print(f\"Model saved to Google Drive at {MODEL_PATH}\")\n",
        "\n",
        "def run_inference_loop():\n",
        "    clear_output()\n",
        "    print(\"Upload an image to predict:\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"No file uploaded, exiting loop.\")\n",
        "        return\n",
        "\n",
        "    img_path = next(iter(uploaded))\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    # Display uploaded image\n",
        "    display(image)\n",
        "\n",
        "    # Predict\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predicted_class = logits.argmax(-1).item()\n",
        "\n",
        "    print(f\"\\nPrediction: {id2label[predicted_class]}\")\n",
        "\n",
        "    button_correct = widgets.Button(description=\"Correct\")\n",
        "    button_wrong = widgets.Button(description=\"Wrong\")\n",
        "\n",
        "    def on_correct_clicked(b):\n",
        "        with output:\n",
        "            clear_output()\n",
        "            print(\"Thanks for confirming the prediction!\")\n",
        "        # After feedback, restart loop to upload new image\n",
        "        run_inference_loop()\n",
        "\n",
        "    def on_wrong_clicked(b):\n",
        "        with output:\n",
        "            clear_output()\n",
        "            print(\"Please provide the correct label (0 for FAKE, 1 for REAL):\")\n",
        "\n",
        "            label_input = widgets.Text(value='', placeholder='Enter 0 or 1', description='Label:')\n",
        "            confirm_button = widgets.Button(description=\"Submit\")\n",
        "            inner_output = widgets.Output()\n",
        "\n",
        "            def on_submit_label(btn):\n",
        "                with inner_output:\n",
        "                    clear_output(wait=True)\n",
        "                    try:\n",
        "                        correct_label = int(label_input.value)\n",
        "                        if correct_label in [0, 1]:\n",
        "                            print(f\"Fine-tuning model on correct label: {id2label[correct_label]}\")\n",
        "                            fine_tune_on_example(image, correct_label)\n",
        "                            # After fine-tuning, ask for next image\n",
        "                            run_inference_loop()\n",
        "                        else:\n",
        "                            print(\"Invalid label! Enter 0 or 1.\")\n",
        "                    except Exception:\n",
        "                        print(\"Please enter a valid integer (0 or 1).\")\n",
        "\n",
        "            confirm_button.on_click(on_submit_label)\n",
        "\n",
        "            display(label_input, confirm_button, inner_output)\n",
        "\n",
        "    button_correct.on_click(on_correct_clicked)\n",
        "    button_wrong.on_click(on_wrong_clicked)\n",
        "\n",
        "    display(button_correct, button_wrong, output)\n",
        "\n",
        "# Start the first iteration\n",
        "run_inference_loop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FzgMzLSelFta",
        "outputId": "5f5f4a43-3db5-4163-8b8a-8b1f25494bee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.32.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
            "Requirement already satisfied: gradio-client==1.10.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.2)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.12)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.2->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.2->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3ba3080ff61b4f9f40.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3ba3080ff61b4f9f40.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "!pip install gradio\n",
        "import gradio as gr\n",
        "from transformers import AutoImageProcessor, SiglipForImageClassification\n",
        "from torch.optim import AdamW\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "# Load model and processor\n",
        "model_name = \"prithivMLmods/deepfake-detector-model-v1\"\n",
        "processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "model = SiglipForImageClassification.from_pretrained(model_name)\n",
        "model.train()\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Labels mapping\n",
        "id2label = {0: \"FAKE\", 1: \"REAL\"}\n",
        "label2id = {\"FAKE\": 0, \"REAL\": 1}\n",
        "\n",
        "# Optimizer for fine-tuning\n",
        "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
        "\n",
        "# Dataset class for single example fine-tuning\n",
        "class SingleImageDataset(Dataset):\n",
        "    def __init__(self, image, label):\n",
        "        self.image = image\n",
        "        self.label = label\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inputs = processor(images=self.image, return_tensors=\"pt\")\n",
        "        inputs = {k: v.squeeze(0) for k,v in inputs.items()}\n",
        "        inputs['labels'] = torch.tensor(self.label)\n",
        "        return inputs\n",
        "\n",
        "def fine_tune(image, correct_label):\n",
        "    dataset = SingleImageDataset(image, correct_label)\n",
        "    dataloader = DataLoader(dataset, batch_size=1)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(1):  # just 1 epoch for fast feedback\n",
        "        for batch in dataloader:\n",
        "            batch = {k: v.to(device) for k,v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    # Save the updated model locally\n",
        "    save_path = \"./fine_tuned_model\"\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "    model.save_pretrained(save_path)\n",
        "    processor.save_pretrained(save_path)\n",
        "    return\n",
        "\n",
        "def predict(image):\n",
        "    model.eval()\n",
        "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        pred_class = logits.argmax(-1).item()\n",
        "    return id2label[pred_class]\n",
        "\n",
        "def inference(image, feedback, correct_label_text):\n",
        "    if image is None:\n",
        "        return \"Please upload an image.\", None\n",
        "\n",
        "    prediction = predict(image)\n",
        "    message = f\"Prediction: {prediction}\"\n",
        "\n",
        "    if feedback == \"Wrong\":\n",
        "        if correct_label_text.upper() in label2id:\n",
        "            correct_label = label2id[correct_label_text.upper()]\n",
        "            fine_tune(image, correct_label)\n",
        "            message += f\" | Model fine-tuned with correct label: {correct_label_text.upper()}\"\n",
        "        else:\n",
        "            message += \" | Please enter a valid correct label (REAL or FAKE).\"\n",
        "\n",
        "    return message, image\n",
        "\n",
        "# Gradio UI setup\n",
        "title = \"Deepfake Detector with Interactive Feedback and Fine-tuning\"\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=inference,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"pil\", label=\"Upload Image\"),\n",
        "        gr.Radio([\"Correct\", \"Wrong\"], label=\"Is the prediction correct?\", value=\"Correct\"),\n",
        "        gr.Textbox(label=\"If Wrong, enter correct label (REAL or FAKE)\", lines=1, placeholder=\"REAL or FAKE\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Output\"),\n",
        "        gr.Image(type=\"pil\", label=\"Uploaded Image\")\n",
        "    ],\n",
        "    title=title,\n",
        "    live=False,\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZuFP9S0XOGq3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "outputId": "187e5fbe-59c0-4745-cc92-801bde932c76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://0f0517d1b6ae43db5e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0f0517d1b6ae43db5e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from transformers import AutoImageProcessor, SiglipForImageClassification\n",
        "from PIL import Image\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Mount Google Drive to save updated model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Model paths\n",
        "drive_model_path = \"/content/drive/MyDrive/deepfake_detector_model\"\n",
        "os.makedirs(drive_model_path, exist_ok=True)\n",
        "\n",
        "# Load model and processor\n",
        "model_name = drive_model_path if os.path.exists(os.path.join(drive_model_path, \"config.json\")) else \"prithivMLmods/deepfake-detector-model-v1\"\n",
        "model = SiglipForImageClassification.from_pretrained(model_name)\n",
        "processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "\n",
        "# Class labels\n",
        "id2label = {0: \"FAKE\", 1: \"REAL\"}\n",
        "label2id = {\"FAKE\": 0, \"REAL\": 1}\n",
        "\n",
        "# For storing last image\n",
        "last_image = None\n",
        "\n",
        "def predict(image):\n",
        "    global last_image\n",
        "    last_image = image\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predicted_class = logits.argmax(-1).item()\n",
        "    prediction = id2label[predicted_class]\n",
        "    return prediction, image\n",
        "\n",
        "def feedback(user_choice):\n",
        "    global last_image\n",
        "    if last_image is None:\n",
        "        return \"No image uploaded yet.\"\n",
        "\n",
        "    inputs = processor(images=last_image, return_tensors=\"pt\")\n",
        "    labels = torch.tensor([label2id[user_choice]]).unsqueeze(0)\n",
        "\n",
        "    # Training step\n",
        "    outputs = model(**inputs, labels=labels)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Save updated model to Google Drive\n",
        "    model.save_pretrained(drive_model_path)\n",
        "    processor.save_pretrained(drive_model_path)\n",
        "\n",
        "    return f\"Model updated with feedback: {user_choice}\"\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## üß† FakeFinder - Deepfake Detection Tool\")\n",
        "    image_input = gr.Image(type=\"pil\", label=\"Upload Image\")\n",
        "    predict_btn = gr.Button(\"Predict\")\n",
        "    result = gr.Textbox(label=\"Prediction\")\n",
        "    image_display = gr.Image(label=\"Image Preview\")\n",
        "\n",
        "    gr.Markdown(\"### Was this prediction correct?\")\n",
        "    correct_btn = gr.Button(\"‚úÖ Yes\")\n",
        "    wrong_btn = gr.Button(\"‚ùå No\")\n",
        "    feedback_output = gr.Textbox(label=\"Feedback Status\")\n",
        "\n",
        "    def on_submit(img):\n",
        "        prediction, preview = predict(img)\n",
        "        return prediction, preview\n",
        "\n",
        "    def on_correct():\n",
        "        return feedback(\"REAL\")\n",
        "\n",
        "    def on_wrong():\n",
        "        return feedback(\"FAKE\")\n",
        "\n",
        "    predict_btn.click(on_submit, inputs=image_input, outputs=[result, image_display])\n",
        "    correct_btn.click(on_correct, outputs=feedback_output)\n",
        "    wrong_btn.click(on_wrong, outputs=feedback_output)\n",
        "\n",
        "demo.launch(share=True)  # share=True gives you a public link\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YHOOpxEAUrG7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}